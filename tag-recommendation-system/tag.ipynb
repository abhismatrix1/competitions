{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "### This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/he-toxic-multilabel\"))\n\n# Any results you write to the current directory are saved as output.\nimport os\nimport gc\nimport keras\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom keras import backend as K\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import roc_auc_score,f1_score\nfrom sklearn.cross_validation import KFold\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, MaxPooling1D, Conv1D, SpatialDropout1D\nfrom keras.layers import add, Dropout, PReLU, BatchNormalization, GlobalMaxPooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import Callback\nfrom keras import optimizers\nfrom keras import initializers, regularizers, constraints, callbacks\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom keras import backend as K\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "067771e5ac7bad03b4690292a0f4132eb47464ae"
      },
      "cell_type": "code",
      "source": "def precision(y_true, y_pred):\n    \"\"\"Precision metric.\n    Only computes a batch-wise average of precision.\n    Computes the precision, a metric for multi-label classification of\n    how many selected items are relevant.\n    \"\"\"\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef recall(y_true, y_pred):\n    \"\"\"Recall metric.\n    Only computes a batch-wise average of recall.\n    Computes the recall, a metric for multi-label classification of\n    how many relevant items are selected.\n    \"\"\"\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef fbeta_score(y_true, y_pred, beta=1):\n    \"\"\"Computes the F score.\n    The F score is the weighted harmonic mean of precision and recall.\n    Here it is only computed as a batch-wise average, not globally.\n    This is useful for multi-label classification, where input samples can be\n    classified as sets of labels. By only using accuracy (precision) a model\n    would achieve a perfect score by simply assigning every class to every\n    input. In order to avoid this, a metric should penalize incorrect class\n    assignments as well (recall). The F-beta score (ranged from 0.0 to 1.0)\n    computes this, as a weighted mean of the proportion of correct class\n    assignments vs. the proportion of incorrect class assignments.\n    With beta = 1, this is equivalent to a F-measure. With beta < 1, assigning\n    correct classes becomes more important, and with beta > 1 the metric is\n    instead weighted towards penalizing incorrect class assignments.\n    \"\"\"\n    if beta < 0:\n        raise ValueError('The lowest choosable beta is zero (only precision).')\n\n    # If there are no true positives, fix the F score at 0 like sklearn.\n    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n        return 0\n\n    p = precision(y_true, y_pred)\n    r = recall(y_true, y_pred)\n    bb = beta ** 2\n    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n    return fbeta_score\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train=pd.read_csv('../input/he-toxic-multilabel/d583b256-d-new_dataset/new_dataset/train.csv')\n#train=train.sample(frac=.4)\ntrain=train.drop(['id'],axis=1)\n\ntrain['tags']=train['tags'].astype(str)\n\n\ntrain['article']=train['article'].str.replace('</p>|<p>|\\r|\\n|<br>|</p>|<pre>|</pre>|<code>|</code>','')\n\ntrain['combined']=train['title']+' '+train['article']\n\ntrain.drop(['title','article'],axis=1,inplace=True)\n\nlst=[x.split(',') for x in train['tags'].str.replace('|',',').tolist()]\n\n\nmlb = MultiLabelBinarizer(sparse_output=True)\ny=mlb.fit_transform(lst)\ndel lst",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e28565817a0922db87ef2631fc2fc74775119b6d"
      },
      "cell_type": "code",
      "source": "test=pd.read_csv('../data/he-toxic-multilabel/d583b256-d-new_dataset/new_dataset/test.csv')\n\ntest=test.drop(['id'],axis=1)\ntest['article']=test['article'].str.replace('</p>|<p>|\\r|\\n|<br>|</p>|<pre>|</pre>|<code>|</code>','')\n\ntest['combined']=test['title']+' '+test['article']\n\ntest.drop(['title','article'],axis=1,inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3a7c0c811047cfc9ee8e88e49092139ae65f14e0"
      },
      "cell_type": "code",
      "source": "\n\ndef schedule(ind):\n    indx=int(ind/8)\n    a = [ 0.001, 0.0005, 0.0003, 0.0002,0.0001,0.0001,0.00005]\n    return a[indx] \nfrom keras.utils import Sequence\nfrom keras.callbacks import ModelCheckpoint ,ReduceLROnPlateau\n\n\nclass XRAYSequence(Sequence):\n\n    def __init__(self, x_set, y_set, batch_size):\n        self.x, self.y = x_set, y_set\n\n    def __len__(self):\n        return int(np.ceil(len(self.x) / float(self.batch_size)))\n\n    def __getitem__(self, idx):\n\n        X_batch = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        y_batch = self.y[idx * self.batch_size:(idx + 1) * self.batch_size].todense()\n\n        return np.vstack(X_batch),np.vstack(y_batch)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e2064c4a4293ea0b301a3c92eb797006ad258e5d"
      },
      "cell_type": "code",
      "source": "EMBEDDING_FILE = '../data/crawl-300d-2M.vec'\n\nX_train = train[\"combined\"].fillna(\"fillna\").values\ny_train = y#train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\ndel y\nX_test = test[\"combined\"].fillna(\"fillna\").values\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "abbd7d791eb362b159c54c5ccf8d656096123e87"
      },
      "cell_type": "code",
      "source": "max_features = 100000\nmaxlen = 200\nembed_size = 300",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "33a8d2173f7d7ac97ce6d27f7ebf9d4062e835a7"
      },
      "cell_type": "code",
      "source": "del test,train",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fe0eb2ccfd85967e3b8fb4b65611a6bc7ac1160b"
      },
      "cell_type": "code",
      "source": "tokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train) + list(X_test))\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\nx_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(X_test, maxlen=maxlen)\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "de6ea464fc22868377eacc435e485985a54c86cc"
      },
      "cell_type": "code",
      "source": "del X_train, X_test",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ecfd5071409b74d63d7b788a42d734fe1bacde9f"
      },
      "cell_type": "code",
      "source": "EMBEDDING_FILE = '../data/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\ndef get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE, encoding=\"utf8\"))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3376fc3da8b06e82662d8e8c7b1c8e8eee541280"
      },
      "cell_type": "code",
      "source": "all_embs = np.stack(embeddings_index.values())\nemb_mean, emb_std = all_embs.mean(), all_embs.std()\n\ndel all_embs #, X_train, X_test, train, test\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ef9c2a0c5240e50dd79565d88ecce2ebae433e11"
      },
      "cell_type": "code",
      "source": "word_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n    \nprint('preprocessing done')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8d6e3ecee38166a9a891038fa99cece3b8efc4bd"
      },
      "cell_type": "code",
      "source": "\n#model\n#wrote out all the blocks instead of looping for simplicity\nfilter_nr = 64\nfilter_size = 3\nmax_pool_size = 3\nmax_pool_strides = 2\ndense_nr = 256\nspatial_dropout = 0.2\ndense_dropout = 0.5\ntrain_embed = False\nconv_kern_reg = regularizers.l2(0.00001)\nconv_bias_reg = regularizers.l2(0.00001)\n\ncomment = Input(shape=(maxlen,))\nemb_comment = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=train_embed)(comment)\nemb_comment = SpatialDropout1D(spatial_dropout)(emb_comment)\n\nblock1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(emb_comment)\nblock1 = BatchNormalization()(block1)\nblock1 = PReLU()(block1)\nblock1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block1)\nblock1 = BatchNormalization()(block1)\nblock1 = PReLU()(block1)\n\n#we pass embedded comment through conv1d with filter size 1 because it needs to have the same shape as block output\n#if you choose filter_nr = embed_size (300 in this case) you don't have to do this part and can add emb_comment directly to block1_output\nresize_emb = Conv1D(filter_nr, kernel_size=1, padding='same', activation='linear', \n            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(emb_comment)\nresize_emb = PReLU()(resize_emb)\n    \nblock1_output = add([block1, resize_emb])\nblock1_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block1_output)\n\nblock2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block1_output)\nblock2 = BatchNormalization()(block2)\nblock2 = PReLU()(block2)\nblock2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block2)\nblock2 = BatchNormalization()(block2)\nblock2 = PReLU()(block2)\n    \nblock2_output = add([block2, block1_output])\nblock2_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block2_output)\n\nblock3 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block2_output)\nblock3 = BatchNormalization()(block3)\nblock3 = PReLU()(block3)\nblock3 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block3)\nblock3 = BatchNormalization()(block3)\nblock3 = PReLU()(block3)\n    \nblock3_output = add([block3, block2_output])\nblock3_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block3_output)\n\nblock4 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block3_output)\nblock4 = BatchNormalization()(block4)\nblock4 = PReLU()(block4)\nblock4 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block4)\nblock4 = BatchNormalization()(block4)\nblock4 = PReLU()(block4)\n\nblock4_output = add([block4, block3_output])\nblock4_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block4_output)\n\nblock5 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block4_output)\nblock5 = BatchNormalization()(block5)\nblock5 = PReLU()(block5)\nblock5 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block5)\nblock5 = BatchNormalization()(block5)\nblock5 = PReLU()(block5)\n\nblock5_output = add([block5, block4_output])\nblock5_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block5_output)\n\nblock6 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block5_output)\nblock6 = BatchNormalization()(block6)\nblock6 = PReLU()(block6)\nblock6 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block6)\nblock6 = BatchNormalization()(block6)\nblock6 = PReLU()(block6)\n\nblock6_output = add([block6, block5_output])\nblock6_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block6_output)\n\nblock7 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block6_output)\nblock7 = BatchNormalization()(block7)\nblock7 = PReLU()(block7)\nblock7 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n            kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block7)\nblock7 = BatchNormalization()(block7)\nblock7 = PReLU()(block7)\n\nblock7_output = add([block7, block6_output])\noutput = GlobalMaxPooling1D()(block7_output)\n\noutput = Dense(dense_nr, activation='linear')(output)\noutput = BatchNormalization()(output)\noutput = PReLU()(output)\noutput = Dropout(dense_dropout)(output)\noutput = Dense(36321, activation='sigmoid',kernel_initializer=keras.initializers.RandomUniform(minval=-3e-4, maxval=3e-4, seed=None))(output)\n#,kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=1, seed=None)\nmodel = Model(comment, output)\n\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "81bfd55b11bc4dbfd40e64966f7c1e096bcdec0c"
      },
      "cell_type": "code",
      "source": "\nmodel.compile(loss='binary_crossentropy', \n            optimizer=optimizers.Adam(lr=.001),\n            metrics=[fbeta_score])\n            \n\nepochs = 40\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "af24e50c10e6026b24c5acecfc3d5cb80343cc88"
      },
      "cell_type": "code",
      "source": "batch_size = 512\nXtrain, Xval, ytrain, yval = train_test_split(x_train, y_train, train_size=0.98, random_state=233)\n\n\n\nlr = callbacks.LearningRateScheduler(schedule)\n\nxtrain_gen=XRAYSequence(x_train,y_train,batch_size)\n\neval_gen=XRAYSequence(Xval,yval,batch_size)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a605d101d7e42d579ef85dc760ee8e1e53d6128a"
      },
      "cell_type": "code",
      "source": "del x_train, y_train",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "358fb8fc13be2fea57f448c90a9abf4b1a6a8cfe"
      },
      "cell_type": "code",
      "source": "checkpoint=callbacks.ModelCheckpoint('model_123',save_best_only=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "trusted": true,
        "_uuid": "929183b58249affa3ed4e62d5e47d8001bb36e7b"
      },
      "cell_type": "code",
      "source": "\nhistory=model.fit_generator(xtrain_gen,\n                        steps_per_epoch=ytrain.shape[0] // batch_size, epochs=epochs,\n                       validation_data=eval_gen,\n                        validation_steps=yval.shape[0] // batch_size,\n                         verbose=1,\n                        use_multiprocessing=False,\n                            callbacks=[lr]\n                       )\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "13aea49883da256d738115e59091de0407af0bbe"
      },
      "cell_type": "code",
      "source": "from keras.models import load_model\nmodel.load_weights('model_123')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ebf21bd3e676c0abb2aec69a9ca2fd07b138feee"
      },
      "cell_type": "code",
      "source": "test=pd.read_csv('../data/he-toxic-multilabel/d583b256-d-new_dataset/new_dataset/test.csv')['id']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3de37d8dc2ae2f49715075e01d48cbff5a837d79"
      },
      "cell_type": "code",
      "source": "\nfrom keras import backend as K\ndef difference(vectors):\n    x,y=vectors\n    return (tf.stop_gradient(y)-x)\n\ndef diff_output_shape(shapes):\n    shape_x,shape_y=shapes\n    return shape_x\n\ndef logits(vector):\n    import tensorflow as tf\n    _epsilon = tf.convert_to_tensor(K.epsilon(), vector.dtype.base_dtype)\n    output = tf.clip_by_value(vector, _epsilon, 1 - _epsilon)\n    output = tf.log(output / (1 - output))\n    #output=output+tf.keras.backend.constant(.4102,vector.dtype.base_dtype)\n    return tf.stop_gradient(output)\n\ndef logit_output_shape(shape):\n    return shape\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f41b01c6013cae7c2a5f2f649c24b37fb2697cc8"
      },
      "cell_type": "code",
      "source": "\nfrom keras.layers import Input, Dense, Lambda, Activation\nfrom keras.models import Model\nfrom keras import backend as K\n\n\n#input_b=Input()\n#input_b=Input(shape=(224,224,3), dtype='float32', name='input')\n\noutput_1=Model(inputs=[model.get_input_at(0)],\n                                      outputs=[model.layers[63].output])\noutput_2=Model(inputs=[model.get_input_at(0)],\n                                      outputs=[model.layers[65].output])\n#x=GlobalAveragePooling2D()(output_1.output)\nx = Dense(512)(output_1.output)\nx=BatchNormalization()(x)\nx= PReLU()(x)\nx = Dense(196)(x)\nx=BatchNormalization()(x)\nx= PReLU()(x)\ntheta=Dense(36321)(x)\n\nlogit=Lambda(logits,output_shape=logit_output_shape,name='logits')(output_2.output)\n\ndiff = Lambda(difference,\n                  output_shape=diff_output_shape,name='diff')([theta, logit])\ndiff=Activation('sigmoid')(diff)\nmodel2=Model(inputs=[model.get_input_at(0)],outputs=diff)\nmodel2.summary()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "da734e39cacca1decb6cc59d942154e8c219b6fb"
      },
      "cell_type": "code",
      "source": "\nlist_layers=[75,74,73,72,70,69,68,67,66,65,64,]\nfor layer in range(len(model2.layers)):\n    model2.layers[layer].trainable=False\nfor layer in list_layers:\n    model2.layers[layer].trainable=True\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4aea8604d9615e35e0ee4b0e7e0aa3e124a8d82a"
      },
      "cell_type": "code",
      "source": "print(\"training started\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cb77ce80744583411a247357e8e27f3ae9ea1e29"
      },
      "cell_type": "code",
      "source": "\nmodel2.compile(loss='binary_crossentropy', \n            optimizer=optimizers.Adam(lr=.001),\n            metrics=[fbeta_score,precision,recall])\n            \n\nepochs = 30\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0703d097341c83903a46bd0dc57661fb25ac3bae"
      },
      "cell_type": "code",
      "source": "checkpoint2=callbacks.ModelCheckpoint('model_D',save_best_only=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "92901bc40a4ea5f0d8df30514184773dcd500acd"
      },
      "cell_type": "code",
      "source": "\nhistory3=model2.fit_generator(xtrain_gen,\n                        steps_per_epoch=ytrain.shape[0] // batch_size, epochs=epochs,\n                       validation_data=eval_gen,\n                        validation_steps=yval.shape[0] // batch_size,\n                         verbose=1,\n                        use_multiprocessing=False,\n                            callbacks=[lr,checkpoint2]\n                       )\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0d9387628a8c59462ecac8744a5af923f7872d6f"
      },
      "cell_type": "code",
      "source": "model2.load_weights('model_D')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ff0db07bff53b0710805c9ba1104c52de067e92c"
      },
      "cell_type": "code",
      "source": "\nthreshold=[.22]\nloop=int(194323/batch_size)\nremained=194323%batch_size\ni=0\n\nfor thre in threshold:\n    pre=[]\n    while True:\n        start=i*batch_size\n        end=(i+1)*batch_size\n        if i==loop:\n            end=i*batch_size+remained\n            batch_size=remained\n            print(start,end)\n        pred=model2.predict(x_test[start:end])\n        ddx=np.argmax(pred,axis=1)\n        pred[np.arange(batch_size),ddx]=1\n        pred[pred>=thre]=1\n        pred[pred<thre]=0\n        tagss=mlb.inverse_transform(pred)\n        for tg,idx in zip(tagss,test.values[start:end]):\n            strn=''\n            for wo in tg:\n                strn=strn+wo+'|'\n            pre.append({'id':idx,'tags':strn[:-1]})\n        i+=1\n\n        if i>loop:\n            break\n\n\npd.DataFrame(pre).to_csv(\"submission.csv\",index=False)\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}